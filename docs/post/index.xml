<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Ciel&#39;s blog website &lt;3</title>
        <link>https://wwwCielwww.github.io/post/</link>
        <description>Recent content in Posts on Ciel&#39;s blog website &lt;3</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 21 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://wwwCielwww.github.io/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>MA4270 Data Modelling and Computation Notes</title>
        <link>https://wwwCielwww.github.io/p/ma4270/</link>
        <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/p/ma4270/</guid>
        <description>&lt;h2 id=&#34;ch-7-boosting&#34;&gt;Ch 7. Boosting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Decision stumps, where $\theta={s,k,\theta_0}$. $k=$ index of feature, $s=$ sign and $\theta_0$ = threshold.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
h(\boldsymbol{x};\boldsymbol{\theta})=\text{sign}(s(x_k-\theta_0))
$$&lt;/p&gt;
&lt;p&gt;Weighted decision function&lt;/p&gt;
&lt;p&gt;$$
f_M(\boldsymbol{x})=\sum_{m=1}^M\alpha_mh(\boldsymbol{x};\boldsymbol{\theta}_m)
$$&lt;/p&gt;
&lt;p&gt;Individual $h$ are called weak/base learners. AdaBoost helps find good ${\boldsymbol{\theta}_m,\alpha_m}$&lt;/p&gt;
&lt;h3 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h3&gt;
&lt;p&gt;$\boldsymbol{\hat{\theta}}_m=\arg\min_\theta\sum_{t:y_t\neq h(\boldsymbol{x};\boldsymbol{\theta})}w_{m-1}(t)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;initialize weights $w_0(t)=\frac{1}{n}$ for $t=1,\dots,n$ ($n=$ size of data set)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for $m=1,\dots,M$ do&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{\hat{\theta}}_m=\arg\min_\theta\sum_{t:y_t\neq h(\boldsymbol{x};\boldsymbol{\theta})}w_{m-1}(t)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\hat{\alpha}_m=\frac{1}{2}\log\frac{1-\hat{\epsilon}_m}{\hat{\epsilon}_m}$, where $\hat{\epsilon}_m$ is the minimal value attained in 2.1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$w_m(t)=\frac{1}{Z_m}w_{m-1}(t)e^{-y_th(\boldsymbol{x}_t;\hat{\boldsymbol{\theta}}_m)\hat{\alpha}_m}$, where $Z_m$ is the sum of all unnormalized $w_{m-1}(t)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;output: $f_M(\boldsymbol{x})=\sum_{m=1}^M\hat{\alpha}_mh(\boldsymbol{x};\hat{\boldsymbol{\theta}}_m) \to\hat{y}=\text{sign}(f_M(\boldsymbol{x}))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: one can observe that the test error still decreases when training error reaches 0. This is because AdaBoost is implicitly minimizing the margin, hence making the classifier more robust.&lt;/p&gt;
&lt;h2 id=&#34;ch-8-concentration&#34;&gt;Ch 8. Concentration&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Hoeffding&amp;rsquo;s inequality&lt;/strong&gt; Let $Z=X_1+\cdots+X_n$, where the $X_i$ are independent and supported on $[a_i,b_i]$. Then&lt;/p&gt;
&lt;p&gt;$$
\mathbb{P}[\frac{1}{n}\vert Z-\mathbb{E}[Z]\vert &amp;gt;\epsilon]\leq2\exp(-\frac{2n\epsilon^2}{\frac{1}{n}\sum_{i=1}^n(b_i-a_i)^2})
$$&lt;/p&gt;
&lt;h2 id=&#34;ch-9-theory&#34;&gt;Ch 9. Theory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mathcal{D}={(\boldsymbol{x}_i,y_i)}^n_{i=1},(\boldsymbol{x}_i,y_i)\sim P_{XY}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;possible classifiers $f(\boldsymbol{x})$ make up the function class $\mathcal{F}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;risk/test error $R(f)=\mathbb{E}[l(y,f(\boldsymbol{x})]$ gives the Bayes-optimal classifier&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f_{erm}=\arg\min_{f\in\mathcal{F}}R_n(f)$ training error $R_n(f)=\frac{1}{n}\sum_{i=1}^nl(y_i,f(\boldsymbol{x}_i))$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;test error = training error + generalization error&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pac-learning&#34;&gt;PAC Learning&lt;/h3&gt;
&lt;p&gt;To ensure small generalization error, PAC (probably approximately correct) learning seeks to attain a risk within a small value of that chieved by the best $f$ in $\mathcal{F}$.&lt;/p&gt;
&lt;p&gt;Given $l$, $\mathcal{F}$ is PAC-learnable if there exists an algorithm $\mathcal{A}(\mathcal{D}_n)$ and a function $\bar{n}(\epsilon,\delta)$ such that: for any $P_{XY}$ and any $\epsilon,\delta\in(0,1)$, if $n\geq\bar{n}(\epsilon,\delta)$, then the following holds with probability at least $1-\delta$:&lt;/p&gt;
&lt;p&gt;$$
R(\hat{f})\leq\min_{f\in\mathcal{F}}R(f)+\epsilon
$$&lt;/p&gt;
&lt;p&gt;$1-\delta\to$ probably correct, $\epsilon\to$ approximately correct, $\bar{n}\to$ sample complexity.&lt;/p&gt;
&lt;h3 id=&#34;finite-function-class&#34;&gt;Finite Function Class&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; for any bounded loss function in $[0,1]$, any finite function class $\mathcal{F}$ is PAC-learnable with sample complexity $\bar{n}(\epsilon,\delta)=\frac{2}{\epsilon^2}\log\frac{2|\mathcal{F}|}{\delta}$.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: taking the algorithm to be ERM (empirical risk minimization), apply Hoeffding&amp;rsquo;s inequality&lt;/p&gt;
&lt;p&gt;$$
P[|R(f)-R_n(f)|\geq\epsilon_0]\leq2e^{-2n\epsilon_0^2}.
$$&lt;/p&gt;
&lt;p&gt;We cannot simply substitute $f=f_{erm}$ because the later is not fixed, but rather a random variable depending on $\mathcal{D}$. For finite $\mathcal{F}$,&lt;/p&gt;
&lt;p&gt;$$
\mathcal{P}[\bigcup_{f\in\mathcal{F}}{|R(f)-R_n(f)|&amp;gt;\epsilon_0}]\leq2|\mathcal{F}|e^{-2n\epsilon_0^2}
$$&lt;/p&gt;
&lt;p&gt;By setting the RHS to a target $\delta$, we find the sufficient $n$ as $\frac{1}{2\epsilon_0^2}\log\frac{2|\mathcal{F}|}{\delta}$. Assume the probability $1-\delta$ event occurs, namely $|R(f)-R_n(f)|\leq\epsilon_0$ for all $f\in\mathcal{F}$. Letting $f^*$ be the function that minimizes $R(f)$, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
R(f_{erm})-R(f^&lt;em&gt;)&amp;amp;=(R(f_{erm})-R_n(f_{erm}))+(R_n(f_{erm})-R_n(f^&lt;/em&gt;))+(R_n(f^&lt;em&gt;)-R(f^&lt;/em&gt;)) \
&amp;amp;\leq\epsilon_0+0+\epsilon_0=2\epsilon_0
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Setting $\epsilon_0=\frac{\epsilon}{2}$ gives the desired bound, and yields $n=\frac{2}{\epsilon^2}\log\frac{2|\mathcal{F}|}{\delta}$ .&lt;/p&gt;
&lt;h3 id=&#34;infinite-case--vc-dimension&#34;&gt;Infinite Case &amp;amp; VC Dimension&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt; even with infinitely many hypotheses, there may be only finitely many effective hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Growth function/shattering number $S_n(\mathcal{F})=\sup_{\boldsymbol{x}_1,\dots,\boldsymbol{x}_n}|{(f(\boldsymbol{x}_1),\dots,f(\boldsymbol{x}_n)):f\in\mathcal{F}}|$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is an integer between $1$ and $2^n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VC dimension $d_{VC}=d_{VC}(\mathcal{F})$ is the largest $k$ such that $S_k(\mathcal{F})=2^k$. If $S_k(\mathcal{F})=2^k$ for all $k$, then we define $d_{VC}=\infty$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A set of such $k$ points is said to be shattered by $\mathcal{F}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Sauer&amp;rsquo;s lemma&lt;/em&gt;: $S_n(\mathcal{F})\leq\sum_{i=1}^{d_{VC}}\begin{pmatrix}n\i\end{pmatrix}$. For $n\leq d_{VC},S_n(\mathcal{F})=2^n$. Otherwise, $S_n(\mathcal{F})\leq(\frac{d_{VC}}{n})^{d_{VC}}$. A slightly weaker bound is $S_n(\mathcal{F})\leq(n+1)^{d_{VC}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; if $d_{VC}&amp;lt;\infty$, then $\mathcal{F}$ is PAC-learnable under the 0-1 loss with sample complexity&lt;/p&gt;
&lt;p&gt;$$
\bar{n}(\epsilon,\delta)=C\cdot\frac{d_{VC}+\log\frac{1}{\epsilon}}{\epsilon^2}
$$&lt;/p&gt;
&lt;p&gt;for some constant $C$. Conversely, if $d_{VC}=\infty$, then $\mathcal{F}$ is not PAC-leranable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hence, $d_{VC}$ serves as a fundamental measure of richness of the function class â€“ to get good generalization, it suffices to have $n\gg d_{VC}$ samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even if $d_{VC}$ is infinite, efficient learning might be possible for a given data distribution $P_{XY}$. The VC theory only establishes the difficulty of worst-case distributions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;rectangular classifier: $d_{VC}=4$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;linear classifier w/ and w/o an offset in dimnension $d$: $d_{VC}=d$ or $d+1$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
