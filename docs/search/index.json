[{"content":"Ch 7. Boosting  Decision stumps, where $\\theta={s,k,\\theta_0}$. $k=$ index of feature, $s=$ sign and $\\theta_0$ = threshold.  $$ h(\\boldsymbol{x};\\boldsymbol{\\theta})=\\text{sign}(s(x_k-\\theta_0)) $$\nWeighted decision function\n$$ f_M(\\boldsymbol{x})=\\sum_{m=1}^M\\alpha_mh(\\boldsymbol{x};\\boldsymbol{\\theta}_m) $$\nIndividual $h$ are called weak/base learners. AdaBoost helps find good ${\\boldsymbol{\\theta}_m,\\alpha_m}$\nAdaBoost $\\boldsymbol{\\hat{\\theta}}_m=\\arg\\min_\\theta\\sum_{t:y_t\\neq h(\\boldsymbol{x};\\boldsymbol{\\theta})}w_{m-1}(t)$\n  initialize weights $w_0(t)=\\frac{1}{n}$ for $t=1,\\dots,n$ ($n=$ size of data set)\n  for $m=1,\\dots,M$ do\n  $\\boldsymbol{\\hat{\\theta}}_m=\\arg\\min_\\theta\\sum_{t:y_t\\neq h(\\boldsymbol{x};\\boldsymbol{\\theta})}w_{m-1}(t)$\n  $\\hat{\\alpha}_m=\\frac{1}{2}\\log\\frac{1-\\hat{\\epsilon}_m}{\\hat{\\epsilon}_m}$, where $\\hat{\\epsilon}_m$ is the minimal value attained in 2.1\n  $w_m(t)=\\frac{1}{Z_m}w_{m-1}(t)e^{-y_th(\\boldsymbol{x}_t;\\hat{\\boldsymbol{\\theta}}_m)\\hat{\\alpha}_m}$, where $Z_m$ is the sum of all unnormalized $w_{m-1}(t)$\n    output: $f_M(\\boldsymbol{x})=\\sum_{m=1}^M\\hat{\\alpha}_mh(\\boldsymbol{x};\\hat{\\boldsymbol{\\theta}}_m) \\to\\hat{y}=\\text{sign}(f_M(\\boldsymbol{x}))$\n  Note: one can observe that the test error still decreases when training error reaches 0. This is because AdaBoost is implicitly minimizing the margin, hence making the classifier more robust.\nCh 8. Concentration Hoeffding\u0026rsquo;s inequality Let $Z=X_1+\\cdots+X_n$, where the $X_i$ are independent and supported on $[a_i,b_i]$. Then\n$$ \\mathbb{P}[\\frac{1}{n}\\vert Z-\\mathbb{E}[Z]\\vert \u0026gt;\\epsilon]\\leq2\\exp(-\\frac{2n\\epsilon^2}{\\frac{1}{n}\\sum_{i=1}^n(b_i-a_i)^2}) $$\nCh 9. Theory   $\\mathcal{D}={(\\boldsymbol{x}_i,y_i)}^n_{i=1},(\\boldsymbol{x}_i,y_i)\\sim P_{XY}$\n  possible classifiers $f(\\boldsymbol{x})$ make up the function class $\\mathcal{F}$\n  risk/test error $R(f)=\\mathbb{E}[l(y,f(\\boldsymbol{x})]$ gives the Bayes-optimal classifier\n  $f_{erm}=\\arg\\min_{f\\in\\mathcal{F}}R_n(f)$ training error $R_n(f)=\\frac{1}{n}\\sum_{i=1}^nl(y_i,f(\\boldsymbol{x}_i))$\n  test error = training error + generalization error\n  PAC Learning To ensure small generalization error, PAC (probably approximately correct) learning seeks to attain a risk within a small value of that chieved by the best $f$ in $\\mathcal{F}$.\nGiven $l$, $\\mathcal{F}$ is PAC-learnable if there exists an algorithm $\\mathcal{A}(\\mathcal{D}_n)$ and a function $\\bar{n}(\\epsilon,\\delta)$ such that: for any $P_{XY}$ and any $\\epsilon,\\delta\\in(0,1)$, if $n\\geq\\bar{n}(\\epsilon,\\delta)$, then the following holds with probability at least $1-\\delta$:\n$$ R(\\hat{f})\\leq\\min_{f\\in\\mathcal{F}}R(f)+\\epsilon $$\n$1-\\delta\\to$ probably correct, $\\epsilon\\to$ approximately correct, $\\bar{n}\\to$ sample complexity.\nFinite Function Class Theorem for any bounded loss function in $[0,1]$, any finite function class $\\mathcal{F}$ is PAC-learnable with sample complexity $\\bar{n}(\\epsilon,\\delta)=\\frac{2}{\\epsilon^2}\\log\\frac{2\\mid \\mathcal{F}\\mid }{\\delta}$.\nProof: taking the algorithm to be ERM (empirical risk minimization), apply Hoeffding\u0026rsquo;s inequality\n$$ P[\\mid R(f)-R_n(f)\\mid \\geq\\epsilon_0]\\leq2e^{-2n\\epsilon_0^2}. $$\nWe cannot simply substitute $f=f_{erm}$ because the later is not fixed, but rather a random variable depending on $\\mathcal{D}$. For finite $\\mathcal{F}$,\n$$ \\mathcal{P}[\\bigcup_{f\\in\\mathcal{F}}{\\mid R(f)-R_n(f)\\mid \u0026gt;\\epsilon_0}]\\leq2\\mid \\mathcal{F}\\mid e^{-2n\\epsilon_0^2} $$\nBy setting the RHS to a target $\\delta$, we find the sufficient $n$ as $\\frac{1}{2\\epsilon_0^2}\\log\\frac{2\\mid \\mathcal{F}\\mid }{\\delta}$. Assume the probability $1-\\delta$ event occurs, namely $\\mid R(f)-R_n(f)\\mid \\leq\\epsilon_0$ for all $f\\in\\mathcal{F}$. Letting $f^*$ be the function that minimizes $R(f)$, we have\n$$ \\begin{align*} R(f_{erm})-R(f^*)\u0026amp;=(R(f_{erm})-R_n(f_{erm}))+(R_n(f_{erm})-R_n(f^*))+(R_n(f^*)-R(f^*)) \\\\ \u0026amp;\\leq\\epsilon_0+0+\\epsilon_0=2\\epsilon_0 \\end{align*} $$\nSetting $\\epsilon_0=\\frac{\\epsilon}{2}$ gives the desired bound, and yields $n=\\frac{2}{\\epsilon^2}\\log\\frac{2\\mid \\mathcal{F}\\mid }{\\delta}$ .\nInfinite Case \u0026amp; VC Dimension Intuition even with infinitely many hypotheses, there may be only finitely many effective hypotheses.\nDefinitions\n  Growth function/shattering number $S_n(\\mathcal{F})=\\sup_{\\boldsymbol{x}_1,\\dots,\\boldsymbol{x}_n}\\mid {(f(\\boldsymbol{x}_1),\\dots,f(\\boldsymbol{x}_n)):f\\in\\mathcal{F}}\\mid $\n This is an integer between $1$ and $2^n$.    VC dimension $d_{VC}=d_{VC}(\\mathcal{F})$ is the largest $k$ such that $S_k(\\mathcal{F})=2^k$. If $S_k(\\mathcal{F})=2^k$ for all $k$, then we define $d_{VC}=\\infty$.\n A set of such $k$ points is said to be shattered by $\\mathcal{F}$.    Sauer\u0026rsquo;s lemma:\n$$ S_n(\\mathcal{F})\\leq\\sum_{i=1}^{d_{VC}}\\begin{pmatrix}n\\\\i\\end{pmatrix} $$\n. For $n\\leq d_{VC},S_n(\\mathcal{F})=2^n$. Otherwise, $S_n(\\mathcal{F})\\leq(\\frac{d_{VC}}{n})^{d_{VC}}$. A slightly weaker bound is $S_n(\\mathcal{F})\\leq(n+1)^{d_{VC}}$.\nTheorem if $d_{VC}\u0026lt;\\infty$, then $\\mathcal{F}$ is PAC-learnable under the 0-1 loss with sample complexity\n$$ \\bar{n}(\\epsilon,\\delta)=C\\cdot\\frac{d_{VC}+\\log\\frac{1}{\\epsilon}}{\\epsilon^2} $$\nfor some constant $C$. Conversely, if $d_{VC}=\\infty$, then $\\mathcal{F}$ is not PAC-leranable.\n  Hence, $d_{VC}$ serves as a fundamental measure of richness of the function class â€“ to get good generalization, it suffices to have $n\\gg d_{VC}$ samples.\n  Even if $d_{VC}$ is infinite, efficient learning might be possible for a given data distribution $P_{XY}$. The VC theory only establishes the difficulty of worst-case distributions.\n  Examples\n  rectangular classifier: $d_{VC}=4$\n  linear classifier w/ and w/o an offset in dimnension $d$: $d_{VC}=d$ or $d+1$\n  Ch 11. Unsupervised Clustering Given the number of clusters $K$, clustering is the task of seeking a partition of the data set $\\mathcal{D}_1\\cup\\dots\\cup\\mathcal{D}_K$, as well as an associated set of cluster centers $\\boldsymbol{\\mu},\\dots,\\boldsymbol{\\mu}_K$, such that the sum of distances\n$$ J({\\mathcal{D}_j}_{j=1}^K, {\\mathcal{\\mu}_j}_{j=1}^K)=\\sum^K_{j=1}\\sum_{\\boldsymbol{x}\\in\\mathcal{D}_j}\\mid \\mid \\boldsymbol{x}-\\boldsymbol{\\mu}_j\\mid \\mid ^2 $$\nare minimized. This problem is NP-hard in general.\nK-means\nMinimize $J$ by alternatively fixing $\\boldsymbol{\\mu}_j$ and $\\mathcal{D}_j$ until convergence.\n $\\mathcal{D}_j={\\boldsymbol{x}\\in\\mathcal{D}:j={\\arg\\min}_{j\u0026rsquo;=1,\\dots,K}\\mid \\mid \\boldsymbol{x}-\\boldsymbol{\\mu}_{j\u0026rsquo;}\\mid \\mid ^2}$ $\\boldsymbol{\\mu}_j=\\frac{1}{\\mid \\mathcal{D}_j}\\sum_{\\boldsymbol{x}\\in\\mathcal{D}_j}\\boldsymbol{x}$.  cannot gaurantee the solution found is optimal, but since $J$ decreases after each iteration, we must converge to a local minimum.\nProblems tends to favor similar-size clusters; cannot detect clusters other than the ball-like ones\nDistribution Learning Parametric methods consider classes of distributions $p(\\boldsymbol{x};\\boldsymbol{\\theta})$. The learning algorithm (MLE) finds some $\\hat{\\boldsymbol{\\theta}}$, and the estimate of the distribution is $\\hat{p}(\\boldsymbol{x})=p(\\boldsymbol{x};\\hat{\\boldsymbol{\\theta}})$, i.e.,\n$$ \\hat{\\boldsymbol{\\theta}}={\\arg\\max}_\\theta\\prod^n_{t=1}p(\\boldsymbol{x}_t;\\boldsymbol{\\theta}) $$\nunbiased estimate: $\\mathbb{E}[\\hat{\\boldsymbol{\\theta}}]=\\boldsymbol{\\theta}^*$\ncompare with the supervised case: $p(y\\mid \\boldsymbol{x};\\boldsymbol{\\theta})$\nNon-parametric methods: K-nearest neighbors (supervised), kernel density estimation (data point increase the density more in the nearby regions, like a smooth version of a histogram)\n","date":"2022-11-21T00:00:00Z","permalink":"https://wwwCielwww.github.io/p/ma4270/","title":"MA4270 Data Modelling and Computation Notes"}]