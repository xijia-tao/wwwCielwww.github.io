<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>ai on Example Site</title>
        <link>https://wwwCielwww.github.io/tags/ai/</link>
        <description>Recent content in ai on Example Site</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 11 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://wwwCielwww.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>COMP3270 Artificial Intelligence Course Notes</title>
        <link>https://wwwCielwww.github.io/p/comp3270-artificial-intelligence-course-notes/</link>
        <pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/p/comp3270-artificial-intelligence-course-notes/</guid>
        <description>&lt;p&gt;The post gives some keypoints on the course content of COMP3270 Artificial Intelligence @HKU, 2021-22 semester 2. It can be used as a directional material for those who are interested in (the more traditional side of) AI, or for revision purposes to future attendants of this course.&lt;/p&gt;
&lt;h2 id=&#34;1-search&#34;&gt;1. Search&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;uninformed search (BFS, DFS, UCS)&lt;/li&gt;
&lt;li&gt;informed search (greedy, A*)
&lt;ul&gt;
&lt;li&gt;A* TSA is optimal iff admissible&lt;/li&gt;
&lt;li&gt;A* GSA is optimal iff consistent (which implies admissible)&lt;/li&gt;
&lt;li&gt;consistency: h(a) - h(c) &amp;lt;= cost(a to c) / f value (sum) along a path never decreases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;local search
&lt;ul&gt;
&lt;li&gt;cost of neighboring states (randomly)&lt;/li&gt;
&lt;li&gt;find local minimum&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;constraint satisfaction - csp
&lt;ul&gt;
&lt;li&gt;backtracking search (DFS, 1 variable at a time, only legal assignments at each point)&lt;/li&gt;
&lt;li&gt;improvements
&lt;ul&gt;
&lt;li&gt;forward checking (cross off values given the current config)&lt;/li&gt;
&lt;li&gt;constraint propagation ac-3 (repeatedly enforce, arc consistency iff some valid y in head for all x in tail)&lt;/li&gt;
&lt;li&gt;variable ordering (mrv -&amp;gt; min remaining values; most degree ~ tie-breaker)&lt;/li&gt;
&lt;li&gt;value ordering (lcv -&amp;gt; least constraining value, rules out the fewest )&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;adversarial search (minimax, dls, utility)
&lt;ul&gt;
&lt;li&gt;horizon effect: unavoidable damage with a low depth limit, delay -&amp;gt; more damage&lt;/li&gt;
&lt;li&gt;$\alpha-\beta$ pruning:
&lt;ul&gt;
&lt;li&gt;$\alpha:=$ best explored option along path to root for max&lt;/li&gt;
&lt;li&gt;initialize $\alpha=-\infty, \beta=\infty$&lt;/li&gt;
&lt;li&gt;max value function: is terminal -&amp;gt; return utility value&lt;/li&gt;
&lt;li&gt;for each action, &lt;code&gt;v = max(v, min-value(s&#39;, alpha, beta))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;if $v\geq\beta$, then return $v$&lt;/li&gt;
&lt;li&gt;&lt;code&gt;alpha=max(alpha, v)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;finally return $v$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;expectimax: replace min nodes with chance nodes by computing the weighted average of children&lt;/li&gt;
&lt;li&gt;expectiminimax: environment is an extra random agent that moves after each min/max agent&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;2-mdp&#34;&gt;2. MDP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;MDP: S, A, T(s, a, s&amp;rsquo;) = P(s&amp;rsquo; \mid  s, a), R(s, a, s&amp;rsquo;), s0, optional terminal state&lt;/li&gt;
&lt;li&gt;stationarity (sequences with the same start state have the same order without it) implies only two ways to assign utilities to sequences
&lt;ul&gt;
&lt;li&gt;additive rewards&lt;/li&gt;
&lt;li&gt;discounted rewards&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$V(s), Q(s, a), \pi(s)$&lt;/li&gt;
&lt;li&gt;time-limited values save computation for no / unreachable terminal states&lt;/li&gt;
&lt;li&gt;value iteration: $V_{k+1}(s)\leftarrow \max_a\sum_{s&amp;rsquo;}T(s, a, s&amp;rsquo;)[R(s, a, s&amp;rsquo;)+\gamma V_{k}(s&amp;rsquo;)]$ with $V_0=0$, repeat until convergence
&lt;ul&gt;
&lt;li&gt;slow: $O(S^2A)$ per iteration&lt;/li&gt;
&lt;li&gt;policy converges long before values&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;policy iteration: do several passes that update utilities with fixed policy; a new policy is chosen with one-step lookahead (like policy extraction)&lt;/li&gt;
&lt;li&gt;policy evaluation: utilities for a fixed policy $V^\pi(s)=\sum_{s&amp;rsquo;}T(s, \pi(s), s&amp;rsquo;)[R(s, \pi, s&amp;rsquo;) + \gamma V^\pi(s&amp;rsquo;)]$ (use method similar to value iteration as above / use linear solver since max is gone)&lt;/li&gt;
&lt;li&gt;policy extraction: (mini-)expectimax on V*, i.e. one-step lookahead / directly from Q&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;3-rl&#34;&gt;3. RL&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;TD-learning
&lt;ul&gt;
&lt;li&gt;sample = $R(s,\pi(s), s&amp;rsquo;)+\gamma V^\pi(s&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;update: $V^\pi(s)\leftarrow (1-\alpha)V^\pi(s)+\alpha[\text{sample}]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q-learning
&lt;ul&gt;
&lt;li&gt;sample = $R(s,a,s&amp;rsquo;)+\gamma\max_{a&amp;rsquo;}Q(s&amp;rsquo;,a&amp;rsquo;)$&lt;/li&gt;
&lt;li&gt;update: $Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha[\text{sample}]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;exploration function
&lt;ul&gt;
&lt;li&gt;epsilon-greedy: explore a fixed amount&lt;/li&gt;
&lt;li&gt;explore areas whose badness is not (yet) established, eventually stop exploring&lt;/li&gt;
&lt;li&gt;$f(u,n)=u+k/n$&lt;/li&gt;
&lt;li&gt;$Q(s,a)\leftarrow R(s,a,s&amp;rsquo;)+\gamma\max_{a&amp;rsquo;}f(Q(s&amp;rsquo;,a&amp;rsquo;),N(s&amp;rsquo;,a&amp;rsquo;))$&lt;/li&gt;
&lt;li&gt;propagate bonus back to states that lead to unknown states&lt;/li&gt;
&lt;li&gt;minimize regret (difference between rewards and optimal rewards)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;approximate Q-learning
&lt;ul&gt;
&lt;li&gt;weights $w_i$ and features $f_i$, linear combination&lt;/li&gt;
&lt;li&gt;difference = $[r+\gamma\max_{a&amp;rsquo;}Q(s&amp;rsquo;,a&amp;rsquo;)]-Q(s,a)$&lt;/li&gt;
&lt;li&gt;update: $w_i\leftarrow w_i+\alpha[\text{difference}]f_i(s,a)$&lt;/li&gt;
&lt;li&gt;pro: experience is summed up in a few powerful numbers&lt;/li&gt;
&lt;li&gt;con: states may share features but actually be very different in value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;4-mm&#34;&gt;4. MM&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;mini-forward algorithm: time t-1 to t&lt;/li&gt;
&lt;li&gt;stationary distributions $P_\infty(X)$&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;5-hmm&#34;&gt;5. HMM&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;definition
&lt;ul&gt;
&lt;li&gt;initial distribution $P(X_1)$&lt;/li&gt;
&lt;li&gt;transitions $P(X_t \mid  X_{t-1})$&lt;/li&gt;
&lt;li&gt;emissions $P(E_t \mid  X_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;belief state $B_t(X)=P(X_t\mid e_1,\dots,e_t)=P(X_t\mid e_{1:t})$
&lt;ul&gt;
&lt;li&gt;passage of time: $B&amp;rsquo;(X_{t+1}):=P(X_{t+1}\mid e_{i:t})=\sum_{x_t}P(X_{t+1}\mid x_t)B(x_t)$&lt;/li&gt;
&lt;li&gt;$B(X_{t+1})\propto P(e_{t+1}\mid X_{t+1})B&amp;rsquo;(X_{t+1})$&lt;/li&gt;
&lt;li&gt;then renormalize $\to$ beliefs reweighted by likelihood of evidence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;particle filtering
&lt;ul&gt;
&lt;li&gt;comes in when the dimension of X too big to use exact inference (e.g. continuous)&lt;/li&gt;
&lt;li&gt;elapse time: $x&amp;rsquo;=\text{sample}(P(X&amp;rsquo;\mid x))$&lt;/li&gt;
&lt;li&gt;observe: $w(x)=P(e\mid x),\ B(x)\propto P(e\mid x)B&amp;rsquo;(X)$ then renormalize&lt;/li&gt;
&lt;li&gt;resample: select prior samples in proportion to their likelihood&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;forward algorithm (sum of paths)
&lt;ul&gt;
&lt;li&gt;$f_t[x_t]=P(x_t,e_{1:t})=P(e_t\mid x_t)\sum_{x_{t-1}}P(x_t\mid x_{t-1})f_{t-1}[x_{t-1}]$&lt;/li&gt;
&lt;li&gt;get most likely explanation by taking argmax over $x_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Viterbi algorithm (best path)
&lt;ul&gt;
&lt;li&gt;take max instead of sum&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;6-bayes-nets&#34;&gt;6. Bayes Nets&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;conditional independence: d-separation&lt;img src=&#34;https://wwwCielwww.github.io/assets/img/comp3270/d-sep.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;7-nlp&#34;&gt;7. NLP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;word2vec
&lt;ul&gt;
&lt;li&gt;iterate through every word of the whole corpus&lt;/li&gt;
&lt;li&gt;predict surrounding words using word vectors
&lt;ul&gt;
&lt;li&gt;$P(o\mid c)=\frac{\exp(u^T_ov_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}$&lt;/li&gt;
&lt;li&gt;$J(\theta)$ cost function, a sum of negative log probabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gradient descent: update all $\theta$ using all windows&lt;/li&gt;
&lt;li&gt;stochastic gradient descent
&lt;ul&gt;
&lt;li&gt;repeatedly sample windows, and update after each one&lt;/li&gt;
&lt;li&gt;$\nabla J(\theta)\in \mathbb{R}^{2dV}$ is sparse (2dV as every word can appear as a center or context word)&lt;/li&gt;
&lt;li&gt;update at most 2m+1 word vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
